---
title: "Topic Modelling of New York Times Article Headlines During the Spanish Flu & COVID-19 Pandemic"
subtitle: "Term paper SURV727"
author: "Anthony Garove and Ujjayini Das"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
references:
- id: Wickham2014
  title: Tidy Data
  author:
  - family: Wickham
    given: Hadley
  container-title: Journal of Statistical Software
  volume: 59
  issue: 10
  page: 1-23
  type: article-journal
  issued:
    year: 2014
- id: Baumer2017
  title: Modern Data Science with R
  author:
  - family: Baumer
    given: Benjamin S.
  - family: Kaplan
    given: Daniel T.
  - family: Horton
    given: Nicholas J.
  type: book
  publisher: Chapman \& Hall/CRC Press.
  issued:
    year: 2017
---

```{r, include = FALSE}
library(knitr)
library(tidyverse)
```

## Introduction

This section outlines the research idea. We can also cite related work here [@Wickham2014; @Baumer2017].

Note that compiled term paper (the PDF) is supposed to be more text-centered than the RMarkdown documents we used in class, i.e. the text sections are more detailed and big or redundant code chunks can be hidden.

  For our final term paper, Ujjayini and I were generally interested in making historical comparisons across different pandemics. Our overarching purpose was to obtain insights about the similarities in major societal issues caused by pandemics, as well as to examine time-era specific differences across pandemics from different points in history.
  
  We specifically chose to compare the Spanish Flu Pandemic of 1918 to the modern COVID-19 pandemic. We were curious to see how two pandemics from different centuries differed in terms of the areas of life they affected across the globe. We also considered that the type of disease in each pandemic may cause differences in how they affected the world (i.e., an influenza versus a coronavirus pandemic). We thought that searching the historical archives of printed media may provide insights to the research question at hand.
  
  We operationalized this research question by asking, "using Latent Dirichlet Allocation, are there differences in the topics that emerge from published New York Times article headlines during the first respective month of the Spanish Flu and COVID-19 pandemic?"

## Data

This section describes the data sources and the data gathering process.

 We used the New York Times Archive API to access N=12,028 article headlines published during the first month of each pandemic. To the best of our knowledge, the Spanish Flu was officially declared a pandemic on 4th March, 1918. The COVID-19 public health crisis was officially declared a pandemic on 11th March, 2020.

 We used the specific endpoints, month and year, while requesting for published article headlines from the New York Times Archive API with an authorised key. The following code was used to retrieve the data:

```{r}
# Building URL-s

base_url1 <- 'https://api.nytimes.com/svc/archive/v1/1918/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request1 <- GET(base_url1)

base_url2 <- 'https://api.nytimes.com/svc/archive/v1/2020/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request2 <- GET(base_url2)

class(request1)
class(request2)

# Getting the Contents as Data Frames

response1 <- content(request1, as = "text", encoding = "UTF-8")
spanishfludata <- fromJSON(response1, flatten = TRUE) %>% data.frame()

response2 <- content(request2, as = "text", encoding = "UTF-8")
coviddata <- fromJSON(response2, flatten = TRUE) %>% data.frame()

# Extract the Subsetted Data 

headline_spanish <- spanishfludata[,19]
headline_covid <- coviddata[,20]
```

We first formatted the JSON objects into two data frames --- one for the Spanish Flu and another for the COVID-19. After that, we extracted the headlines from both of the data frames and used those extracted objects for the remaining analysis.



## Results



### Pre Processing 

Next, we pre-processed the headline texts using standard text mining procedures in order to prepare the data for topic modelling. These processes included steps such as tokenization, lemmatization and removing stopwords. After performing these steps,two corpora of texts were produced; one for the Spanish Flu headlines and another for the COVID-19 headlines.  

```{r}
# Tokenization

headline_spanish_gsub <- gsub("'","",headline_spanish)
headline_covid_gsub <- gsub("â€™","",headline_covid)
token_spanish <- tokens(headline_spanish_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

token_covid <- tokens(headline_covid_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

# Lemmatization
library(haven)
lemmaData <- read_sav("./lemma_spss.sav")

corpus_spanish <-  tokens_replace(token_spanish, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 


corpus_covid <-  tokens_replace(token_covid, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

# Removing Stopwords
corpus_spanish <- corpus_spanish %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

corpus_covid <- corpus_covid %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)
```

```{r}
# What happens here depends on the specific project
```

### Analysis

This section presents the main results, such as (for example) stats and graphs that show relationships, model results and/or clustering, PCA, etc.

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

## Discussion

This section summarizes the results and may briefly outline advantages and limitations of the work presented.

## References
