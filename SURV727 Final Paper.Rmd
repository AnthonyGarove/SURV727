---
title: "Topic Modelling of New York Times Article Headlines During the Spanish Flu & COVID-19 Pandemic"
subtitle: "Term paper SURV727"
author: "Anthony Garove and Ujjayini Das"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
references:
- id: Wickham2014
  title: Tidy Data
  author:
  - family: Wickham
    given: Hadley
  container-title: Journal of Statistical Software
  volume: 59
  issue: 10
  page: 1-23
  type: article-journal
  issued:
    year: 2014
- id: Baumer2017
  title: Modern Data Science with R
  author:
  - family: Baumer
    given: Benjamin S.
  - family: Kaplan
    given: Daniel T.
  - family: Horton
    given: Nicholas J.
  type: book
  publisher: Chapman \& Hall/CRC Press.
  issued:
    year: 2017
---

```{r, include = FALSE}
library(knitr)
library(tidyverse)
library('httr')
library('jsonlite')
library(quanteda)
library(topicmodels)
library(ggplot2)
library(reshape2)
library(tidytext)
```

## Introduction

This section outlines the research idea. We can also cite related work here [@Wickham2014; @Baumer2017].

Note that compiled term paper (the PDF) is supposed to be more text-centered than the RMarkdown documents we used in class, i.e. the text sections are more detailed and big or redundant code chunks can be hidden.

  For our final term paper, Ujjayini and I were generally interested in making historical comparisons across different pandemics. Our overarching purpose was to obtain insights about the similarities in major societal issues caused by pandemics, as well as to examine time-era specific differences across pandemics from different points in history.
  
  We specifically chose to compare the Spanish Flu Pandemic of 1918 to the modern COVID-19 pandemic. We were curious to see how two pandemics from different centuries differed in terms of the areas of life they affected across the globe. We also considered that the type of disease in each pandemic may cause differences in how they affected the world (i.e., an influenza versus a coronavirus pandemic). We thought that searching the historical archives of printed media may provide insights to the research question at hand.
  
  We operationalized this research question by asking, "using Latent Dirichlet Allocation, are there differences in the topics that emerge from published New York Times article headlines during the first respective month of the Spanish Flu and COVID-19 pandemic?"

## Data

This section describes the data sources and the data gathering process.

 We used the New York Times Archive API to access N=12,028 article headlines published during the first month of each pandemic. To the best of our knowledge, the Spanish Flu was officially declared a pandemic on 4th March, 1918. The COVID-19 public health crisis was officially declared a pandemic on 11th March, 2020.

 We used the specific endpoints, month and year, while requesting for published article headlines from the New York Times Archive API with an authorized key. The following code was used to retrieve the data:

```{r}
# Building URLs
base_url1 <- 'https://api.nytimes.com/svc/archive/v1/1918/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request1 <- GET(base_url1)
base_url2 <- 'https://api.nytimes.com/svc/archive/v1/2020/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request2 <- GET(base_url2)

# Getting the Contents as Data Frames
response1 <- content(request1, as = "text", encoding = "UTF-8")
spanishfludata <- fromJSON(response1, flatten = TRUE) %>% data.frame()
response2 <- content(request2, as = "text", encoding = "UTF-8")
coviddata <- fromJSON(response2, flatten = TRUE) %>% data.frame()

# Extract the Subsetted Data 
headline_spanish <- spanishfludata[,19]
headline_covid <- coviddata[,20]
```

  We first formatted the JSON objects into two data frames --- one for the Spanish Flu and another for the COVID-19. After that, we extracted the headlines from both of the data frames and used those extracted objects for the remaining analysis.

## Results

### Pre Processing 

  Next, we pre-processed the headline texts using standard text mining procedures in order to prepare the data for topic modelling. These processes included tokenization, lemmatization and removing stopwords.Tokenization is a method used in natural language processing that separates text data into units called "tokens". Tokenization can be done to break up data in to words, characters, and subwords. We used n=1 gram tokenization, which separates our headline data into one-word texts. 
  
  Lemmatization is a process that normalizes text such that words are substituted with their base root. This step is used to categorize words into groups of root forms with similar meanings. Stopwords are words that are frequently used in a given language, and these words are removed prior to topic modelling. This step is necessary because stopwords contribute noise when performing Latent Dirichlet Allocation, and removing stopwards allows researchers to interpret the topics extracted from text with more coherence. For both lemmatization and removal of stopwords, we used the standard English dictionary lemmas and stopwords.

  After performing these pre-processing techniques, two corpora of texts were produced; one for Spanish Flu headlines and another for COVID-19 headlines. Based on the two corpora, we then created two document term matrices---where an individual row represents a single document, a column represents a single term, and each value in a matrix contains the frequency of that term in the document. Document-term-matrices are used as inputs for topic modelling with the `topicmodels` package. The code below was used for tokenization, lemmatization, removal of stopwards, and creation of document-term-matrices:

```{r}
# Tokenization
headline_spanish_gsub <- gsub("'","",headline_spanish)
headline_covid_gsub <- gsub("â€™","",headline_covid)
token_spanish <- tokens(headline_spanish_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()
token_covid <- tokens(headline_covid_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

# Lemmatization
library(haven)
lemmaData <- read_sav("./lemma_spss.sav")

corpus_spanish <-  tokens_replace(token_spanish, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 
corpus_covid <-  tokens_replace(token_covid, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

# Removing Stopwords
corpus_spanish <- corpus_spanish %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)
corpus_covid <- corpus_covid %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

#Create document term matrices
DTM_spanish <- dfm(corpus_spanish)
minimumFrequency <- 10
DTM_spanish <- dfm_trim(DTM_spanish, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)
DTM_spanish  <- dfm_select(DTM_spanish, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_spanish) <- stringi::stri_replace_all_regex(colnames(DTM_spanish), 
                                                 "[^_a-z]","")
DTM_spanish <- dfm_compress(DTM_spanish, "features")
sel_idx_spanish <- rowSums(DTM_spanish) > 0
DTM_spanish <- DTM_spanish[sel_idx_spanish, ]
corpus_spanish <- corpus_spanish[sel_idx_spanish, ]
DTM_covid <- dfm(corpus_covid)
minimumFrequency <- 10
DTM_covid <- dfm_trim(DTM_covid, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)
DTM_covid  <- dfm_select(DTM_covid, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_covid) <- stringi::stri_replace_all_regex(colnames(DTM_covid), 
                                                 "[^_a-z]","")
DTM_covid <- dfm_compress(DTM_covid, "features")
sel_idx_covid <- rowSums(DTM_covid) > 0
DTM_covid <- DTM_covid[sel_idx_covid, ]
corpus_covid <- corpus_covid[sel_idx_covid, ]
```

### Topic Modeling Using Latent Dirichlet Allocation

  We used Latent Dirichlet Allocation (LDA) to examine topics from our two corpora of headlines. LDA is a frequently used algorithm for fitting a topic model. LDA is based on an assumption that documents used in your analysis are comprised of a mixture of topics, and that every topic is an amalgamation of words. LDA simultaneously finds the words associated with each topic while extracting the topics contained in each document.
  
#### LDA for Spanish Flu Headlines

   Our initial LDA model was set to extract 20 topics with $\alpha = 3.33$ from the corpus containing New York Times article headlines during the first month of the Spanish Flu Pandemic. After examining the initial results, we reset our model to extract 15 topics with $\alpha$ to 0.2 to make the topics more comprehensible.   
  
  Next we observed the probability distributions of the 15 topics over all the headlines, given by `theta_spanish2` as well as the probability distribution of the words over all the 15 topics, given by `beta_spanish2`. To have a better understanding of the topics, we specifically looked into the five terms with the highest per-topic-per word probabilities within each topic.  
  
```{r}
K_spanish <- 15
set.seed(1234)
topicModel_spanish2 <- LDA(DTM_spanish, 
                   K_spanish, 
                   method="Gibbs", 
                   control=list(iter = 500, 
                                verbose = 25, 
                                alpha = 0.2))
tmResult_spanish2 <- posterior(topicModel_spanish2)
theta_spanish2 <- tmResult_spanish2$topics
beta_spanish2 <- tmResult_spanish2$terms
top5termsPerTopic_spanish2 <- terms(topicModel_spanish2, 
                           5)
topicNames_spanish2 <- apply(top5termsPerTopic_spanish2, 
                    2, 
                    paste, 
                    collapse=" ")
topicProportions_spanish2 <- colSums(theta_spanish2) / nrow(DTM_spanish)  # average probability over all headlines
names(topicProportions_spanish2) <- topicNames_spanish2     # Topic Names 
sort(topicProportions_spanish2, decreasing = TRUE) # sort
topicNames_spanish2 <- apply(terms(topicModel_spanish2, 5), 2, paste, collapse = " ")  # top five terms per topic 
```


#### LDA for COVID-19 Headlines

 Our initial LDA model was set to extract 20 topics with $\alpha = 5$ from the corpus containing New York Times article headlines during the first month of the COVID-19 Pandemic. After examining the initial results, we reset our model to extract 10 topics with $\alpha$ to 0.15 to make the topics more interpretable. At this point, we observed that the word `Coronavirus` appeared in all 10 topics with the highest $\beta$-- suggesting that `Coronavirus` is potentially a stopword specific to this corpus of text. 
 
 Next we removed the term `Coronavirus` from the corpus and fit the LDA model again. After observing the results from this revised model, we ultimately specified the model to extract 15 topics with $\alpha = 0.15$. This was done because while observing the top five terms for each topic, different combinations of `K` topics and $\alpha$ values repeatedly produced more than 5 terms with equal $\beta$-s for many topics. Our final specifications seemed to produce a more parsimonious topic model. 

```{r include=FALSE}
headline_covid_gsub2 <- gsub("Coronavirus","",headline_covid_gsub)
token_covid_gsub2 <- tokens(headline_covid_gsub2,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()
```

```{r include=FALSE}
corpus_covid_gsub2 <-  tokens_replace(token_covid_gsub2,
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 
corpus_covid_gsub2 <- corpus_covid_gsub2 %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)
```

```{r include=FALSE}
DTM_covid_gsub2 <- dfm(corpus_covid_gsub2)
minimumFrequency <- 10
DTM_covid_gsub2 <- dfm_trim(DTM_covid_gsub2, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)
DTM_covid_gsub2  <- dfm_select(DTM_covid_gsub2, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_covid_gsub2) <- stringi::stri_replace_all_regex(colnames(DTM_covid_gsub2), 
                                                 "[^_a-z]","")
DTM_covid_gsub2 <- dfm_compress(DTM_covid_gsub2, "features")
sel_idx_covid_gsub2 <- rowSums(DTM_covid_gsub2) > 0
DTM_covid_gsub2 <- DTM_covid_gsub2[sel_idx_covid_gsub2, ]
corpus_covid_gsub2 <- corpus_covid_gsub2[sel_idx_covid_gsub2, ]
```

```{r}
K_covid_gsub2 <- 15
set.seed(1234)
topicModel_covid_gsub2 <- LDA(DTM_covid_gsub2, 
                  K_covid_gsub2,
                  method="Gibbs", 
                  control=list(iter = 500,
                               verbose = 25,
                               alpha = .15))
tmResult_covid_gsub2 <- posterior(topicModel_covid_gsub2)
beta_covid_gsub2 <- tmResult_covid_gsub2$terms
glimpse(beta_covid_gsub2)            
theta_covid_gsub2 <- tmResult_covid_gsub2$topics
glimpse(theta_covid_gsub2)               
terms(topicModel_covid_gsub2, 10)
top5termsPerTopic_covid_gsub2 <- terms(topicModel_covid_gsub2, 
                           5)
topicNames_covid_gsub2 <- apply(top5termsPerTopic_covid_gsub2, 
                    2, 
                    paste, 
                    collapse=" ")
```


### Visualizations

 To assist with the interpretation of our topic models, we visualized the per-topic-per-term probabilities for both the Spanish Flu and the COVID-19 corpora. We created horizontal bar graphs for each topic; the bars indicating the probabilities of those five words that were most probable to appear within each topic. 
 
```{r}
### Spanish Flu
## Per Topic Term Distribution
spanishflu_topics <- tidy(topicModel_spanish2,matrix = "beta")
spanishflu_topics
top_terms_spanish <- spanishflu_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)
top_terms_spanish_graph <- top_terms_spanish %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
   theme(text = element_text(size=10),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00,0.2)+
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 5) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for Spanish Flu")
print(top_terms_spanish_graph)
```

```{r}
## COVID
## Per Topic Term Distribution
covid_topics_gsub2 <- tidy(topicModel_covid_gsub2,matrix = "beta")
covid_topics_gsub2
top_terms_covid_gsub2 <- covid_topics_gsub2 %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)
top_terms_covid_graph_gsub2 <-top_terms_covid_gsub2 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  theme(text = element_text(size=7),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00, 0.30) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 5) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for COVID")
print(top_terms_covid_graph_gsub2)
```

## Discussion

### Topic Model of NYT Article Headlines from the First Month of The Spanish Flu Pandemic

  Our topic model of published NYT article headlines was specified to extract 15 topics. Topic one is comprised of words specifically related to the legislative process in the United States. Topic two is a bit more ambiguous than topic one, but has the words "war" and "president", suggesting that this topic might be related to World War I (WWI). Topic three appears to be about New York City (NYC), and also contains the word "obituary" and "title". Topic three could perhaps be reflective of obituaries of people from NYC. Topics four and fourteen are both comprised of words that could either describe sports or war. Topics five through seven all appear to be related to the US economy, markets, and goods. Topics eight through ten are similarly comprised of different nations involved in WWI, as well as other war-related terms. Topic eleven is similar to topics eight through ten, insofar as it appears to be related to WWI and contains a nation involved in the war (i.e., "American"). However, topic eleven does not seem to be as clearly related to an easily identifiable topic. Topic twelve consists of terms that mostly seem related to judicial processes. Topic thirteen is not so easy to interpret, though it appears to be gendered and related to women in some way. Topic thirteen could potentially be related to the woman's suffrage movement, which took place around the time of the Spanish Flu Pandemic and World War I. Topic fifteen is, again, related to WWI---and specifically comprised of terms that could be used when describing the army. Overall, it would appear the majority of the topics that were extracted from this corpus of headlines seem to be related to WWI, as well as different aspects of society affected by the war.

### Topic Model of NYT Article Headlines from the First Month of The COVID-19 Pandemic 

Discuss covid-19 topic model/charts

### Overall Discussion

Compare the two, overall summary of findings

## References
