---
title: "SURV727 Final Project"
author: "Anthony Garove and Ujjayini Das"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r}
if (!requireNamespace("httr", quietly = TRUE)) {
  install.packages("httr")
}
if (!requireNamespace("jsonlite", quietly = TRUE)) {
  install.packages("jsonlite")
}

library('httr')
library('jsonlite')
library(tidyverse)
library(quanteda)
library(topicmodels)
```

## URL and Query

```{r}
base_url1 <- 'https://api.nytimes.com/svc/archive/v1/1918/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request1 <- GET(base_url1)

base_url2 <- 'https://api.nytimes.com/svc/archive/v1/2020/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request2 <- GET(base_url2)

class(request1)
class(request2)
```

```{r}
# Check to see if we were successful
request1$status_code
request1$url

request2$status_code
request2$url

```

# Getting the Content as Data Frames

```{r}
response1 <- content(request1, as = "text", encoding = "UTF-8")
spanishfludata <- fromJSON(response1, flatten = TRUE) %>% data.frame()

response2 <- content(request2, as = "text", encoding = "UTF-8")
coviddata <- fromJSON(response2, flatten = TRUE) %>% data.frame()
```

# Extract Subsetted Data

```{r}
headline_spanish <- spanishfludata[,19]
headline_covid <- coviddata[,20]
```

# Text Mining

```{r}

# Tokenization

headline_spanish_gsub <- gsub("'","",headline_spanish)
headline_covid_gsub <- gsub("â€™","",headline_covid)
token_spanish <- tokens(headline_spanish_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

token_covid <- tokens(headline_covid_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

# Lemmatization
library(haven)
lemmaData <- read_sav("./lemma_spss.sav")

```

## Make Corpora

```{r}
corpus_spanish <-  tokens_replace(token_spanish, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_spanish <- corpus_spanish %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

corpus_covid <-  tokens_replace(token_covid, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_covid <- corpus_covid %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

```

## Create DTM - Spanish Flu

```{r}

#  Create dtm
DTM_spanish <- dfm(corpus_spanish)

# Minimum
minimumFrequency <- 10
DTM_spanish <- dfm_trim(DTM_spanish, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)

# keep only letters... brute force
DTM_spanish  <- dfm_select(DTM_spanish, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_spanish) <- stringi::stri_replace_all_regex(colnames(DTM_spanish), 
                                                 "[^_a-z]","")

DTM_spanish <- dfm_compress(DTM_spanish, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx_spanish <- rowSums(DTM_spanish) > 0
DTM_spanish <- DTM_spanish[sel_idx_spanish, ]
corpus_spanish <- corpus_spanish[sel_idx_spanish, ]

```

