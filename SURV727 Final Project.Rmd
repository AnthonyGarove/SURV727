---
title: "SURV727 Final Project"
author: "Anthony Garove and Ujjayini Das"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r}
if (!requireNamespace("httr", quietly = TRUE)) {
  install.packages("httr")
}
if (!requireNamespace("jsonlite", quietly = TRUE)) {
  install.packages("jsonlite")
}

library('httr')
library('jsonlite')
library(tidyverse)
library(quanteda)
library(topicmodels)
```

## URL and Query

```{r}
base_url1 <- 'https://api.nytimes.com/svc/archive/v1/1918/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request1 <- GET(base_url1)

base_url2 <- 'https://api.nytimes.com/svc/archive/v1/2020/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request2 <- GET(base_url2)

class(request1)
class(request2)
```

```{r}
# Check to see if we were successful
request1$status_code
request1$url

request2$status_code
request2$url

```

# Getting the Content as Data Frames

```{r}
response1 <- content(request1, as = "text", encoding = "UTF-8")
spanishfludata <- fromJSON(response1, flatten = TRUE) %>% data.frame()

response2 <- content(request2, as = "text", encoding = "UTF-8")
coviddata <- fromJSON(response2, flatten = TRUE) %>% data.frame()
```

# Extract Subsetted Data

```{r}
headline_spanish <- spanishfludata[,19]
headline_covid <- coviddata[,20]
```

# Text Mining

```{r}

# Tokenization

headline_spanish_gsub <- gsub("'","",headline_spanish)
headline_covid_gsub <- gsub("â€™","",headline_covid)
token_spanish <- tokens(headline_spanish_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

token_covid <- tokens(headline_covid_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

# Lemmatization
library(haven)
lemmaData <- read_sav("./lemma_spss.sav")

```

## Make Corpora

```{r}
corpus_spanish <-  tokens_replace(token_spanish, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_spanish <- corpus_spanish %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

corpus_covid <-  tokens_replace(token_covid, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_covid <- corpus_covid %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

```

## Create DTM - Spanish Flu

```{r}

#  Create dtm
DTM_spanish <- dfm(corpus_spanish)

# Minimum
minimumFrequency <- 10
DTM_spanish <- dfm_trim(DTM_spanish, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)

# keep only letters... brute force
DTM_spanish  <- dfm_select(DTM_spanish, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_spanish) <- stringi::stri_replace_all_regex(colnames(DTM_spanish), 
                                                 "[^_a-z]","")

DTM_spanish <- dfm_compress(DTM_spanish, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx_spanish <- rowSums(DTM_spanish) > 0
DTM_spanish <- DTM_spanish[sel_idx_spanish, ]
corpus_spanish <- corpus_spanish[sel_idx_spanish, ]

```

## Create DTM - Covid

```{r}

#  Create dtm
DTM_covid <- dfm(corpus_covid)

# Minimum
minimumFrequency <- 10
DTM_covid <- dfm_trim(DTM_covid, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)

# keep only letters... brute force
DTM_covid  <- dfm_select(DTM_covid, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_covid) <- stringi::stri_replace_all_regex(colnames(DTM_covid), 
                                                 "[^_a-z]","")

DTM_covid <- dfm_compress(DTM_covid, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx_covid <- rowSums(DTM_covid) > 0
DTM_covid <- DTM_covid[sel_idx_covid, ]
corpus_covid <- corpus_covid[sel_idx_covid, ]

```

## Topic models

```{r}



## Spanish Flu

K_spanish <- 15
# Set seed to make results reproducible
set.seed(1234)
topicModel_spanish <- LDA(DTM_spanish, 
                  K_spanish, 
                  method="Gibbs", 
                  control=list(iter = 500, 
                               verbose = 25))


tmResult_spanish <- posterior(topicModel_spanish)


# Topics are distributions over the entire vocabulary

beta_spanish <- tmResult_spanish$terms
glimpse(beta_spanish)            

# Each doc has a distribution over k topics

theta_spanish <- tmResult_spanish$topics
glimpse(theta_spanish)               

terms(topicModel_spanish, 10)

# Top terms per topic;Using top 5 to interpret topics
top5termsPerTopic_spanish <- terms(topicModel_spanish, 
                           5)

topicNames_spanish <- apply(top5termsPerTopic_spanish, 
                    2, 
                    paste, 
                    collapse=" ")



```

```{r}
## Covid

K_covid <- 10
# Set seed to make results reproducible
set.seed(1234)
topicModel_covid <- LDA(DTM_covid, 
                  K_covid, 
                  method="Gibbs", 
                  control=list(iter = 500, 
                               verbose = 25))

tmResult_covid <- posterior(topicModel_covid)


# Topics are distributions over the entire vocabulary

beta_covid <- tmResult_covid$terms
glimpse(beta_covid)            

# Each doc has a distribution over k topics

theta_covid <- tmResult_covid$topics
glimpse(theta_covid)               

terms(topicModel_covid, 10)

# Top terms per topic;Using top 5 to interpret topics
top5termsPerTopic_covid <- terms(topicModel_covid, 
                           5)

topicNames_covid <- apply(top5termsPerTopic_covid, 
                    2, 
                    paste, 
                    collapse=" ")
```

## What are the most common topics? Spanish flu

```{r}

topicProportions_spanish <- colSums(theta_spanish) / nrow(DTM_spanish)  # average probability over all headlines
names(topicProportions_spanish) <- topicNames_spanish     # Topic Names
sort(topicProportions_spanish, decreasing = TRUE) # sort
```

## What are the most common topics? Covid
```{r}

topicProportions_covid <- colSums(theta_covid) / nrow(DTM_covid)  # average probability over all headlines
names(topicProportions_covid) <- topicNames_covid     # Topic Names
sort(topicProportions_covid, decreasing = TRUE) # sort
```

## Modifying alpha value - spanish flu

```{r}
# What was the value in the previous model?
attr(topicModel_spanish, "alpha") ## 3.33

# Re-estimate model with alpha set by us
topicModel_spanish2 <- LDA(DTM_spanish, 
                   K_spanish, 
                   method="Gibbs", 
                   control=list(iter = 500, 
                                verbose = 25, 
                                alpha = 0.2))
tmResult_spanish2 <- posterior(topicModel_spanish2)
theta_spanish2 <- tmResult_spanish2$topics
beta_spanish2 <- tmResult_spanish2$terms


topicProportions_spanish2 <- colSums(theta_spanish2) / nrow(DTM_spanish)  # average probability over all paragraphs
names(topicProportions_spanish2) <- topicNames_spanish     # Topic Names 
sort(topicProportions_spanish2, decreasing = TRUE) # sort

topicNames_spanish2 <- apply(terms(topicModel_spanish2, 5), 2, paste, collapse = " ")  # top five terms per topic 
```


## Modifying alpha value - Covid

```{r}
# What was the value in the previous model?
attr(topicModel_covid, "alpha")  ## 5

# Re-estimate model with alpha set by us
topicModel_covid2 <- LDA(DTM_covid, 
                   K_covid, 
                   method="Gibbs", 
                   control=list(iter = 500, 
                                verbose = 25, 
                                alpha = 0.15))
tmResult_covid2 <- posterior(topicModel_covid2)
theta_covid2 <- tmResult_covid2$topics
beta_covid2 <- tmResult_covid2$terms


topicProportions_covid2 <- colSums(theta_covid2) / nrow(DTM_covid)  # average probability over all paragraphs
names(topicProportions_covid2) <- topicNames_covid     # Topic Names 
sort(topicProportions_covid2, decreasing = TRUE) # sort

topicNames_covid2 <- apply(terms(topicModel_covid2, 5), 2, paste, collapse = " ")  # top five terms per topic 
```

## Visualization

```{r}
### Spanish Flu

## Per Topic Term Distribution
library(ggplot2)
library(reshape2)
library(tidytext)
spanishflu_topics <- tidy(topicModel_spanish2,matrix = "beta")
spanishflu_topics

top_terms_spanish <- spanishflu_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_spanish_graph <- top_terms_spanish %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
   theme(text = element_text(size=10),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00,0.15)+
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 5) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for Spanish Flu")

print(top_terms_spanish_graph)
```

```{r}
## COVID

## Per Topic Term Distribution

covid_topics <- tidy(topicModel_covid2,matrix = "beta")
covid_topics

top_terms_covid <- covid_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_covid %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00, 0.2) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 4) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for COVID")

```
