---
title: "SURV727 Final Project"
author: "Anthony Garove and Ujjayini Das"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r}
if (!requireNamespace("httr", quietly = TRUE)) {
  install.packages("httr")
}
if (!requireNamespace("jsonlite", quietly = TRUE)) {
  install.packages("jsonlite")
}

library('httr')
library('jsonlite')
library(tidyverse)
library(quanteda)
library(topicmodels)
```

## URL and Query

```{r}
base_url1 <- 'https://api.nytimes.com/svc/archive/v1/1918/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request1 <- GET(base_url1)

base_url2 <- 'https://api.nytimes.com/svc/archive/v1/2020/3.json?api-key=SVyAXhJrhFVCMF4tvGU2GZY3jm1greFU'
request2 <- GET(base_url2)

class(request1)
class(request2)
```

```{r}
# Check to see if we were successful
request1$status_code
request1$url

request2$status_code
request2$url

```

# Getting the Content as Data Frames

```{r}
response1 <- content(request1, as = "text", encoding = "UTF-8")
spanishfludata <- fromJSON(response1, flatten = TRUE) %>% data.frame()

response2 <- content(request2, as = "text", encoding = "UTF-8")
coviddata <- fromJSON(response2, flatten = TRUE) %>% data.frame()
```

# Extract Subsetted Data

```{r}
headline_spanish <- spanishfludata[,19]
headline_covid <- coviddata[,20]
```

# Text Mining

```{r}

# Tokenization

headline_spanish_gsub <- gsub("'","",headline_spanish)
headline_covid_gsub <- gsub("â€™","",headline_covid)
token_spanish <- tokens(headline_spanish_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

token_covid <- tokens(headline_covid_gsub,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

# Lemmatization
library(haven)
lemmaData <- read_sav("./lemma_spss.sav")

```

## Make Corpora

```{r}
corpus_spanish <-  tokens_replace(token_spanish, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_spanish <- corpus_spanish %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

corpus_covid <-  tokens_replace(token_covid, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_covid <- corpus_covid %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)

```

## Create DTM - Spanish Flu

```{r}

#  Create dtm
DTM_spanish <- dfm(corpus_spanish)

# Minimum
minimumFrequency <- 10
DTM_spanish <- dfm_trim(DTM_spanish, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)

# keep only letters... brute force
DTM_spanish  <- dfm_select(DTM_spanish, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_spanish) <- stringi::stri_replace_all_regex(colnames(DTM_spanish), 
                                                 "[^_a-z]","")

DTM_spanish <- dfm_compress(DTM_spanish, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx_spanish <- rowSums(DTM_spanish) > 0
DTM_spanish <- DTM_spanish[sel_idx_spanish, ]
corpus_spanish <- corpus_spanish[sel_idx_spanish, ]

```

## Create DTM - Covid

```{r}

#  Create dtm
DTM_covid <- dfm(corpus_covid)

# Minimum
minimumFrequency <- 10
DTM_covid <- dfm_trim(DTM_covid, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)

# keep only letters... brute force
DTM_covid  <- dfm_select(DTM_covid, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_covid) <- stringi::stri_replace_all_regex(colnames(DTM_covid), 
                                                 "[^_a-z]","")

DTM_covid <- dfm_compress(DTM_covid, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx_covid <- rowSums(DTM_covid) > 0
DTM_covid <- DTM_covid[sel_idx_covid, ]
corpus_covid <- corpus_covid[sel_idx_covid, ]

```

## Topic models

```{r}



## Spanish Flu

K_spanish <- 15
# Set seed to make results reproducible
set.seed(1234)
topicModel_spanish <- LDA(DTM_spanish, 
                  K_spanish, 
                  method="Gibbs", 
                  control=list(iter = 500, 
                               verbose = 25))


tmResult_spanish <- posterior(topicModel_spanish)


# Topics are distributions over the entire vocabulary

beta_spanish <- tmResult_spanish$terms
glimpse(beta_spanish)            

# Each doc has a distribution over k topics

theta_spanish <- tmResult_spanish$topics
glimpse(theta_spanish)               

terms(topicModel_spanish, 10)

# Top terms per topic;Using top 5 to interpret topics
top5termsPerTopic_spanish <- terms(topicModel_spanish, 
                           5)

topicNames_spanish <- apply(top5termsPerTopic_spanish, 
                    2, 
                    paste, 
                    collapse=" ")



```

```{r}
## Covid

K_covid <- 10
# Set seed to make results reproducible
set.seed(1234)
topicModel_covid <- LDA(DTM_covid, 
                  K_covid, 
                  method="Gibbs", 
                  control=list(iter = 500, 
                               verbose = 25))

tmResult_covid <- posterior(topicModel_covid)


# Topics are distributions over the entire vocabulary

beta_covid <- tmResult_covid$terms
glimpse(beta_covid)            

# Each doc has a distribution over k topics

theta_covid <- tmResult_covid$topics
glimpse(theta_covid)               

terms(topicModel_covid, 10)

# Top terms per topic;Using top 5 to interpret topics
top5termsPerTopic_covid <- terms(topicModel_covid, 
                           5)

topicNames_covid <- apply(top5termsPerTopic_covid, 
                    2, 
                    paste, 
                    collapse=" ")
```

## What are the most common topics? Spanish flu

```{r}

topicProportions_spanish <- colSums(theta_spanish) / nrow(DTM_spanish)  # average probability over all headlines
names(topicProportions_spanish) <- topicNames_spanish     # Topic Names
sort(topicProportions_spanish, decreasing = TRUE) # sort
```

## What are the most common topics? Covid
```{r}

topicProportions_covid <- colSums(theta_covid) / nrow(DTM_covid)  # average probability over all headlines
names(topicProportions_covid) <- topicNames_covid     # Topic Names
sort(topicProportions_covid, decreasing = TRUE) # sort
```

## Modifying alpha value - spanish flu

```{r}
# What was the value in the previous model?
attr(topicModel_spanish, "alpha") ## 3.33

# Re-estimate model with alpha set by us
topicModel_spanish2 <- LDA(DTM_spanish, 
                   K_spanish, 
                   method="Gibbs", 
                   control=list(iter = 500, 
                                verbose = 25, 
                                alpha = 0.2))
tmResult_spanish2 <- posterior(topicModel_spanish2)
theta_spanish2 <- tmResult_spanish2$topics
beta_spanish2 <- tmResult_spanish2$terms


topicProportions_spanish2 <- colSums(theta_spanish2) / nrow(DTM_spanish)  # average probability over all paragraphs
names(topicProportions_spanish2) <- topicNames_spanish     # Topic Names 
sort(topicProportions_spanish2, decreasing = TRUE) # sort

topicNames_spanish2 <- apply(terms(topicModel_spanish2, 5), 2, paste, collapse = " ")  # top five terms per topic 
```


## Modifying alpha value - Covid

```{r}
# What was the value in the previous model?
attr(topicModel_covid, "alpha")  ## 5

# Re-estimate model with alpha set by us
topicModel_covid2 <- LDA(DTM_covid, 
                   K_covid, 
                   method="Gibbs", 
                   control=list(iter = 500, 
                                verbose = 25, 
                                alpha = 0.15))
tmResult_covid2 <- posterior(topicModel_covid2)
theta_covid2 <- tmResult_covid2$topics
beta_covid2 <- tmResult_covid2$terms


topicProportions_covid2 <- colSums(theta_covid2) / nrow(DTM_covid)  # average probability over all paragraphs
names(topicProportions_covid2) <- topicNames_covid     # Topic Names 
sort(topicProportions_covid2, decreasing = TRUE) # sort

topicNames_covid2 <- apply(terms(topicModel_covid2, 5), 2, paste, collapse = " ")  # top five terms per topic 
```


## Visualization

```{r}
### Spanish Flu

## Per Topic Term Distribution
library(ggplot2)
library(reshape2)
library(tidytext)
spanishflu_topics <- tidy(topicModel_spanish2,matrix = "beta")
spanishflu_topics

top_terms_spanish <- spanishflu_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_spanish_graph <- top_terms_spanish %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
   theme(text = element_text(size=10),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00,0.15)+
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 5) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for Spanish Flu")

print(top_terms_spanish_graph)
```

```{r}
## COVID

## Per Topic Term Distribution

covid_topics <- tidy(topicModel_covid2,matrix = "beta")
covid_topics

top_terms_covid <- covid_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_covid_graph <-top_terms_covid %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  theme(text = element_text(size=10),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00, 0.2) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 4) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for COVID")

print(top_terms_covid_graph)

```
## Updates post-presentation

```{r}
# Tokenization

headline_covid_gsub2 <- gsub("Coronavirus","",headline_covid_gsub)

token_covid_gsub2 <- tokens(headline_covid_gsub2,
                        remove_punct = TRUE,
                        remove_numbers = TRUE,
                        remove_symbols = TRUE) %>%
  tokens_tolower()

```

```{r}
corpus_covid_gsub2 <-  tokens_replace(token_covid_gsub2, # "Substitute token types based on vectorized one-to-one matching"
                                    lemmaData$inflected_form, 
                                    lemmaData$lemma,
                                    valuetype = "fixed") 

corpus_covid_gsub2 <- corpus_covid_gsub2 %>%
                             tokens_remove(stopwords("english")) %>%
                             tokens_ngrams(1)
```

```{r}

#  Create dtm
DTM_covid_gsub2 <- dfm(corpus_covid_gsub2)

# Minimum
minimumFrequency <- 10
DTM_covid_gsub2 <- dfm_trim(DTM_covid_gsub2, 
                min_docfreq = minimumFrequency,
                max_docfreq = 1000000)

# keep only letters... brute force
DTM_covid_gsub2  <- dfm_select(DTM_covid_gsub2, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM_covid_gsub2) <- stringi::stri_replace_all_regex(colnames(DTM_covid_gsub2), 
                                                 "[^_a-z]","")

DTM_covid_gsub2 <- dfm_compress(DTM_covid_gsub2, "features")

# We have several rows which do not have any content left. Drop them.

sel_idx_covid_gsub2 <- rowSums(DTM_covid_gsub2) > 0
DTM_covid_gsub2 <- DTM_covid_gsub2[sel_idx_covid_gsub2, ]
corpus_covid_gsub2 <- corpus_covid_gsub2[sel_idx_covid_gsub2, ]

```

```{r}
## Covid_gsub2

K_covid_gsub2 <- 15
# Set seed to make results reproducible
set.seed(1234)
topicModel_covid_gsub2 <- LDA(DTM_covid_gsub2, 
                  K_covid_gsub2,
                  method="Gibbs", 
                  control=list(iter = 500,
                               verbose = 25,
                               alpha = .15))

tmResult_covid_gsub2 <- posterior(topicModel_covid_gsub2)


# Topics are distributions over the entire vocabulary

beta_covid_gsub2 <- tmResult_covid_gsub2$terms
glimpse(beta_covid_gsub2)            

# Each doc has a distribution over k topics

theta_covid_gsub2 <- tmResult_covid_gsub2$topics
glimpse(theta_covid_gsub2)               

terms(topicModel_covid_gsub2, 10)

# Top terms per topic;Using top 5 to interpret topics
top5termsPerTopic_covid_gsub2 <- terms(topicModel_covid_gsub2, 
                           5)

topicNames_covid_gsub2 <- apply(top3termsPerTopic_covid_gsub2, 
                    2, 
                    paste, 
                    collapse=" ")
```

```{r}
## COVID

## Per Topic Term Distribution

covid_topics_gsub2 <- tidy(topicModel_covid_gsub2,matrix = "beta")
covid_topics_gsub2

top_terms_covid_gsub2 <- covid_topics_gsub2 %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_covid_graph_gsub2 <-top_terms_covid_gsub2 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  theme(text = element_text(size=7),
        axis.text.x = element_text(angle=60, hjust=1)) + 
  xlim(0.00, 0.30) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol= 4) +
  scale_y_reordered()+
  ggtitle("Distribution of Top 5 Terms per Topic for COVID")

print(top_terms_covid_graph_gsub2)

```

## Discussion

### Topic Model of NYT Article Headlines from the First Month of The Spanish Flu Pandemic

  Our topic model of published NYT article headlines was specified to extract 15 topics. Topic 1 is comprised of words specifically related to the legislative process in the United States. Topic 2 is a bit more ambiguous than topic one, but has the words "war" and "president", suggesting that this topic might be related to World War I (WWI). Topic 3 appears to be about New York City (NYC), and also contains the word "obituary" and "title". Topic 3 could perhaps be reflective of obituaries of people from NYC. Topics 4 and 14 are both comprised of words that could either describe sports or war. Topics 5 through 7 all appear to be related to the US economy, markets, and goods. Topics 8 through 10 are similarly comprised of different nations involved in WWI, as well as other war-related terms. Topic 11 is similar to topics 8 through 10, insofar as it appears to be related to WWI and contains a nation involved in the war (i.e., "American"). However, topic 11 does not seem to be as clearly related to an easily identifiable topic. Topic 12 consists of terms that mostly seem related to judicial processes. Topic 13 is not so easy to interpret, though it appears to be gendered and related to women in some way. Topic 13 could potentially be related to the woman's suffrage movement, which took place around the time of the Spanish Flu Pandemic and World War I. It's also noteworthy that there are two terms for this particular topic that have the same probabilities ("wed" and "honor" with a $\beta$ value of 0.017). Topic 15 is, again, related to WWI---and specifically comprised of terms that could be used when describing the army. Overall, it would appear the majority of the topics that were extracted from this corpus of headlines seem to be related to WWI, as well as different aspects of society affected by the war.

### Topic Model of NYT Article Headlines from the First Month of The COVID-19 Pandemic 

 Our topic model of published NYT article headlines was specified to extract 15 topics from the COVID-19 corpus as well. Out of these 15 topics, 6 topics are not so coherent. These topics include topic 2,4,5,6,9 and 15. The top 5 terms do not convey a clear understanding about these topics. Among the rest, Topic 1 seems to describe the COVID-19 stimulus package introduced by the Trump administration. Topic 3 comprises of terms that are clearly related to the COVID-19 pandemic. Topic 7 could be specifically related to the New York City. Topic 8 is clearly related to the 2020 US Presidential election. Topic 9 could be associated to the pandemic in terms of the effect of pandemic on the market and businesses. Topic 10 could potentially describe a briefing of the markets, although this topic is also not exactly clear. Topic 11 and 12 are clearly associated to the COVID-19 pandemic. Topic 13 might be indirectly related to the COVID-19 pandemic, with words such as "doctor" and "march" in the top 5 terms that we looked at. Topic 14 might relate to the President's response to the pandemic in some way. Overall, the topics that were clearly interpretable, seem to be related to the COVID-19 pandemic as well as different dimensions of it. Also, we had one topic that addressed the 2020 US Presidential election. 


### Overall Discussion

 
